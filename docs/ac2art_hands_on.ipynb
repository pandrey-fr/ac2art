{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on ac2art in a few minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If this fails, check out that dependencies are satisfied.\n",
    "# Also check your config.json file in the package's folder.\n",
    "import ac2art"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing a corpus\n",
    "\n",
    "\n",
    "We'll here use the mocha-timit corpus, whose support is implemented under `ac2art.copora.mocha`. Note that the latter must be explicitely imported to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ac2art.corpora import mocha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running MFCC computation with abkhazia...\n",
      "Successfully ran mfcc computation with abkhazia.\n",
      "Successfully ran scp to txt conversion with kaldi.\n",
      "Succesfully extracted 920 utterances' data to .npy files.\n",
      "Done producing raw MFCC coefficients with abkhazia.\n",
      "22:46:13 : Done with utterance msak0_460.\n"
     ]
    }
   ],
   "source": [
    "# Produce the acoustic and articulatory features.\n",
    "# EMA data and binary voicing are always produced,\n",
    "# however acoustic features must be specified.\n",
    "\n",
    "# Here, we only use the abkhazia-computed MFCC features.\n",
    "# For this function (and all others), use python's `help`\n",
    "# function to access a detailed documentation on how to\n",
    "# tweak it.\n",
    "\n",
    "# Note that when used in console, the timing prints\n",
    "# overwrite each other.\n",
    "\n",
    "mocha.preprocess.extract_utterances_data('mfcc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute normalized versions of the features.\n",
    "# The function must be used for each feature type,\n",
    "# and can be tweaked to use alternative normalization\n",
    "# parameters.\n",
    "\n",
    "# The following parameters are equivalent to using CMVN\n",
    "# parameters in abkhazia: use the latter when producing\n",
    "# features to work with out of the supported corpora.\n",
    "\n",
    "# Note that this will NOT overwrite existing normalization\n",
    "# values generated with similar arguments.\n",
    "# To overwrite them, call `mocha.preprocess.compute_moments`\n",
    "# before (or delete the files containing the parameters).\n",
    "\n",
    "mocha.preprocess.normalize_files('mfcc', norm_type='stds', scope='speaker')\n",
    "mocha.preprocess.normalize_files('ema', norm_type='stds', scope='speaker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the corpus in train/validation/test sets.\n",
    "# The split ensures that triphones coverage is respected\n",
    "# among the subsets. Additionally, when there are multiple\n",
    "# speakers for the corpus, the same utterances are used in\n",
    "# each subset for the various speakers.\n",
    "\n",
    "mocha.preprocess.split_corpus(pct_train=70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using data from a corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implemented under the `ac2art.corpora.<corpus>.load` submodule, `load_audio` and `load_ema` are fully modular functions to load either kind\n",
    "of representations for a given utterance. However, a more practical way of loading the data is to set up a pool of arguments that will be used as (overridable) default arguments by other data loading functions wrapping the former (`load_utterance` and `load_dataset`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'audio_type': 'mfcc_stds_byspeaker',\n",
       " 'articulators': 'all',\n",
       " 'context_window': 5,\n",
       " 'dynamic_ema': True,\n",
       " 'ema_norm': 'mean_byspeaker',\n",
       " 'zero_padding': True}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read-only access to the default arguments.\n",
    "mocha.load.get_loading_setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'audio_type': 'mfcc_stds_byspeaker',\n",
       " 'articulators': 'all',\n",
       " 'context_window': 0,\n",
       " 'dynamic_ema': True,\n",
       " 'ema_norm': 'stds_byspeaker',\n",
       " 'zero_padding': True}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change some of the arguments.\n",
    "# For this, just pass non-None values to them.\n",
    "\n",
    "mocha.load.change_loading_setup(\n",
    "    ema_norm='stds_byspeaker', context_window=0\n",
    ")\n",
    "mocha.load.get_loading_setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(132, 48) (132, 43)\n"
     ]
    }
   ],
   "source": [
    "# Example of utterance data loading.\n",
    "# The arguments used are (by default)\n",
    "# set by the \"loading setup\" dict.\n",
    "\n",
    "acoustic, articulatory = mocha.load.load_utterance('fsew0_001')\n",
    "print(acoustic.shape, articulatory.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(644,) (644,)\n"
     ]
    }
   ],
   "source": [
    "# To load a full subset of utterances.\n",
    "# The resulting data are two flat numpy arrays\n",
    "# (acoustic / articulatory data respectively)\n",
    "# containing 2-D numpy arrays of data, each\n",
    "# associated to an utterance.\n",
    "\n",
    "# Use `get_utterances_list` to... well, get the list\n",
    "# of utterances of the corpus, or of one of the subsets.\n",
    "\n",
    "x_train, y_train = mocha.load.load_dataset('train')\n",
    "x_valid, y_valid = mocha.load.load_dataset('validation')\n",
    "print(x_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up and using a neural network for ac2art inversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we focus on the end-to-end approach.\n",
    "# All other implemented classes of models\n",
    "# (so far) inherit from this one.\n",
    "\n",
    "from ac2art.networks import MultilayerPerceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = MultilayerPerceptron(\n",
    "    input_shape=(None, None, 48),  # batches of time sequences of input vectors made of 48 coefficients\n",
    "    n_targets=15,                  # 14 continuous targets + 1 binary track (voicing)\n",
    "    binary_tracks=[14],            # last output dimension is binary\n",
    "    use_dynamic=True,              # add delta and deltadelta to the continuous targets (total dim: 43)\n",
    "    layers_config=[                # hidden layers stack:\n",
    "        ('dense_layer', 300),      #    fully-connected layer of 300 units with default (relu) activation\n",
    "        ('bi_rnn_stack', 300)      #    bidirectional RNN with single layers of 300 units of default activation,\n",
    "                                   #    default cell-type (lstm) and default aggregation (concatenate)\n",
    "    ],\n",
    "    top_filter=(                   # filter the outputs using a fixed lowpass frequency of 20 Hz\n",
    "        'lowpass_filter', 20, {'learnable': False}\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.11681165,  0.32188216, -0.09232032, ...,  0.01175005,\n",
       "         0.0210929 ,  0.02007481],\n",
       "       [ 0.19524243,  0.39777154, -0.12937222, ...,  0.0157961 ,\n",
       "         0.02269749,  0.01722598],\n",
       "       [ 0.28528634,  0.4140218 , -0.14360052, ...,  0.01892811,\n",
       "         0.02223003,  0.01154162],\n",
       "       ...,\n",
       "       [ 0.16779609,  0.23129499,  0.13174754, ..., -0.0017896 ,\n",
       "         0.00045088, -0.04568204],\n",
       "       [ 0.1097638 ,  0.18255684,  0.14121087, ..., -0.00273296,\n",
       "        -0.00193995, -0.03676203],\n",
       "       [ 0.06221345,  0.12735417,  0.11843851, ..., -0.00432341,\n",
       "        -0.00389861, -0.02653102]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Process one (or more) input vectors - i.e. invert them.\n",
    "# Note: here the results are uninteresting, as the model\n",
    "# has not been trained yet.\n",
    "\n",
    "rnn.predict(x_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9393352 , 0.9238293 , 1.1329341 , 0.87665   , 1.1511712 ,\n",
       "        1.1322352 , 1.5790507 , 1.0087804 , 1.09318   , 0.9669999 ,\n",
       "        1.2592554 , 1.2350925 , 0.65715945, 0.55236495, 0.5742287 ,\n",
       "        0.11269346, 0.10926371, 0.10734623, 0.10094201, 0.11920125,\n",
       "        0.09529941, 0.06640099, 0.11326186, 0.08011833, 0.0745648 ,\n",
       "        0.06267537, 0.13867272, 0.06208453, 0.05442723, 0.0305159 ,\n",
       "        0.03192915, 0.02956705, 0.02804492, 0.03618066, 0.02752729,\n",
       "        0.02645637, 0.02847949, 0.02768154, 0.02494742, 0.02511004,\n",
       "        0.03402368, 0.0351573 , 0.0294042 ]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Score on one (or more) utterance.\n",
    "\n",
    "rnn.score(x_valid[0], y_valid[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.93933517, 0.92382926, 1.1329341 , 0.87665   , 1.1511712 ,\n",
       "        1.1322353 , 1.5790509 , 1.0087804 , 1.09318   , 0.9669999 ,\n",
       "        1.2592555 , 1.2350925 , 0.6571595 , 0.55236495, 0.5742287 ,\n",
       "        0.11269345, 0.10926372, 0.10734624, 0.10094201, 0.11920125,\n",
       "        0.09529941, 0.06640099, 0.11326185, 0.08011833, 0.0745648 ,\n",
       "        0.06267537, 0.13867272, 0.06208453, 0.05442725, 0.03051589,\n",
       "        0.03192915, 0.02956706, 0.02804492, 0.03618066, 0.02752729,\n",
       "        0.02645637, 0.0284795 , 0.02768154, 0.02494742, 0.02511004,\n",
       "        0.03402368, 0.0351573 , 0.0294042 ],\n",
       "       [0.9110082 , 1.2083749 , 1.0382272 , 1.021703  , 1.0071024 ,\n",
       "        1.192074  , 0.61097956, 0.9199053 , 1.2847241 , 1.2516372 ,\n",
       "        0.74275976, 0.91278875, 0.39567268, 0.78875756, 0.5710404 ,\n",
       "        0.09672309, 0.09738339, 0.09096681, 0.10522888, 0.09947834,\n",
       "        0.08568613, 0.04871767, 0.06752534, 0.06458824, 0.08320254,\n",
       "        0.06654856, 0.12617496, 0.04241813, 0.03431138, 0.02548349,\n",
       "        0.02690453, 0.03448188, 0.02592869, 0.0263384 , 0.02932536,\n",
       "        0.031597  , 0.01995833, 0.03282181, 0.01948332, 0.02612908,\n",
       "        0.033068  , 0.02362861, 0.03150701]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For score or predict, if the model is set up\n",
    "# for batch processing, you may pass a list of\n",
    "# utterance data and get a similar output.\n",
    "# The implementation takes care of zero-padding,\n",
    "# avoiding useless computations on the pads and\n",
    "# removing them in the end.\n",
    "\n",
    "rnn.score(x_valid[:2], y_valid[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.92950416, 1.03156981, 1.10099993, 0.92955761, 1.10328573,\n",
       "       1.15329568, 1.32569464, 0.97892711, 1.16319145, 1.07440933,\n",
       "       1.1075561 , 1.13369805, 0.57989904, 0.64439732, 0.57312193,\n",
       "       0.10713908, 0.10513674, 0.10164146, 0.10241399, 0.11234034,\n",
       "       0.09196135, 0.06027194, 0.09737282, 0.07476444, 0.07756144,\n",
       "       0.06404831, 0.13433413, 0.05521093, 0.04744617, 0.02871928,\n",
       "       0.03017358, 0.03125241, 0.02714093, 0.03272434, 0.02815488,\n",
       "       0.02826203, 0.02547628, 0.02952454, 0.02303402, 0.02558742,\n",
       "       0.03369197, 0.03112822, 0.03004007])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To compute a scoring metrics over an iterable\n",
    "# of utterances, use score_corpus.\n",
    "\n",
    "rnn.score_corpus(x_valid[:2], y_valid[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a training step, using some input / output\n",
    "# vector(s).\n",
    "\n",
    "rnn.run_training_function(x_train[:2], y_train[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step   0: [1.02978767 1.06072456 0.98860962 1.10188606 1.18272216 1.30777922\n",
      " 1.02376768 0.91330324 1.16536146 0.98272865 1.50169419 1.00942094\n",
      " 1.73960479 2.46286461 0.58550236 0.10208998 0.11745203 0.09855865\n",
      " 0.1080503  0.1004002  0.10037166 0.07912243 0.08744027 0.06995177\n",
      " 0.08590896 0.09131701 0.12749374 0.07887209 0.07203317 0.03217299\n",
      " 0.03263936 0.03188306 0.02505049 0.03192314 0.02903864 0.02360825\n",
      " 0.02817176 0.02425792 0.0212578  0.02957154 0.03363444 0.03118113\n",
      " 0.04180009]\n",
      "Step  20: [0.86497475 0.80858884 0.83272146 0.87148201 0.8509646  0.72916925\n",
      " 0.90430639 0.76306025 0.91811251 0.86246549 0.97012825 0.79723529\n",
      " 0.85910932 0.93779301 0.35138728 0.08955047 0.0976416  0.08224566\n",
      " 0.09176718 0.08457348 0.07618139 0.06889063 0.06859979 0.06323359\n",
      " 0.07398233 0.08094213 0.09633651 0.06750751 0.05592361 0.04564988\n",
      " 0.0606576  0.04556537 0.04400237 0.04660961 0.06413935 0.03622612\n",
      " 0.05571635 0.02378946 0.04429782 0.02989118 0.07068502 0.04077778\n",
      " 0.03127039]\n",
      "Step  40: [0.7962107  0.75316408 0.79445428 0.83152635 0.79658559 0.65550984\n",
      " 0.87068602 0.76402085 0.87645958 0.83779881 0.93465641 0.75140366\n",
      " 0.84171555 0.89681769 0.305622   0.08438584 0.09242552 0.0782701\n",
      " 0.08784581 0.07934392 0.07204763 0.06508586 0.06448496 0.06191823\n",
      " 0.07076535 0.07837777 0.09081017 0.06489465 0.05343343 0.05169091\n",
      " 0.06128183 0.05026849 0.05076268 0.05187862 0.06816066 0.03941558\n",
      " 0.06147455 0.02914267 0.04559753 0.03580315 0.08281044 0.04228575\n",
      " 0.03154928]\n",
      "Step  60: [0.71736841 0.70774596 0.7273064  0.80531525 0.74141668 0.65659459\n",
      " 0.8515095  0.67613476 0.92648189 0.79709235 0.89297544 0.71378203\n",
      " 0.81328138 0.9822666  0.28560615 0.07950794 0.08675828 0.07410777\n",
      " 0.08548771 0.07528097 0.06966479 0.06389456 0.06275793 0.06083823\n",
      " 0.06793418 0.0758807  0.08685181 0.06324805 0.0525411  0.05889873\n",
      " 0.08127221 0.0619584  0.05892805 0.06183071 0.0694272  0.04249587\n",
      " 0.06602594 0.03175689 0.05036223 0.0398644  0.08629712 0.04281317\n",
      " 0.03202911]\n",
      "Step  80: [0.6901312  0.68533263 0.72783301 0.76856431 0.71045084 0.61798905\n",
      " 0.83467813 0.65845695 0.86088595 0.78365044 0.88383313 0.68520631\n",
      " 0.7995227  0.89651007 0.28088478 0.07688176 0.08559019 0.07175261\n",
      " 0.0823078  0.07242758 0.06731032 0.06265648 0.06310908 0.06006674\n",
      " 0.06667429 0.07403393 0.08442742 0.06070323 0.05114117 0.06612826\n",
      " 0.0769072  0.06339702 0.0592778  0.06492964 0.07034276 0.04330717\n",
      " 0.06380722 0.03168368 0.05109351 0.0463121  0.08787919 0.04809754\n",
      " 0.03477727]\n"
     ]
    }
   ],
   "source": [
    "# Here is an example of (shortened) training loop.\n",
    "\n",
    "# You may want to add further actions on the regular\n",
    "# score check-ups, such as focusing on parts of the\n",
    "# score output, de-normalizing it, checking if it is\n",
    "# going better, saving the model and/or triggering\n",
    "# early stopping critieria...\n",
    "\n",
    "# Here, we train on random batches of ten utterances\n",
    "# and check what the (raw) scores are every twenty\n",
    "# batches.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "for i in range(100):\n",
    "    if not i % 20:\n",
    "        scores = rnn.score_corpus(x_valid, y_valid)\n",
    "        print('Step %3i:' % i, scores)\n",
    "    batch = np.random.choice(len(x_train), replace=False, size=10)\n",
    "    rnn.run_training_function(x_train[batch], y_train[batch])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving and restoring models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model to a .npy file.\n",
    "\n",
    "rnn.save_model('dummy_rnn.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the model's weights randomly.\n",
    "\n",
    "rnn.reset_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restore the model's weights from a previous state.\n",
    "\n",
    "rnn.restore_model('dummy_rnn.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-instantiate and restore the model.\n",
    "\n",
    "rnn = ac2art.networks.load_dumped_model('dummy_rnn.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the inversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function run_inversion in module ac2art._invert:\n",
      "\n",
      "run_inversion(source, inverter, destination, keep_channels=None)\n",
      "    Run acoustic-to-articulatory inversion of a set of features.\n",
      "    \n",
      "    Requires pre-computed acoustic features and a pre-trained\n",
      "    acoustic-to-articulatory inverter neural network.\n",
      "    \n",
      "    source        : path to the **normalized** input features, which may\n",
      "                    be stored as a single ark, scp or ark-like txt file,\n",
      "                    or as npy files in a given folder\n",
      "    inverter      : NeuralNetwork-inheriting instance, or path to\n",
      "                    a .npy file recording a dumped model of such kind\n",
      "    destination   : path where to output the inverted features, which\n",
      "                    may be written as .npy files in a given folder or\n",
      "                    compiled in a .ark, .scp or ark-like .txt file\n",
      "    keep_channels : optional list of indexes of channel of inverted\n",
      "                    features to keep (default None, implying all)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This requires to have produced normalized acoustic features\n",
    "# and trained a neural network for ac2art inversion.\n",
    "\n",
    "# Do read the docs to see which data formats may be used as inputs\n",
    "# and outputs.\n",
    "\n",
    "help(ac2art.run_inversion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract features to compute ABX metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function extract_h5_features in module ac2art.corpora.prototype.abx._abx:\n",
      "\n",
      "extract_h5_features(audio_features=None, ema_features=None, inverter=None, output_name='mocha_features', articulators=None, dynamic_ema=True, sampling_rate=100)\n",
      "    Build an h5 file recording audio features associated with mocha data.\n",
      "    \n",
      "    audio_features : optional name of audio features to use, including\n",
      "                     normalization indications\n",
      "    ema_features   : optional name of ema features' normalization to use\n",
      "                     (use '' for raw data and None for no EMA data)\n",
      "    inverter       : optional acoustic-articulatory inverter whose\n",
      "                     predictions to use, based on the audio features\n",
      "    output_name    : base name of the output file (default 'mocha_features')\n",
      "    articulators   : optional list of articulators to keep among EMA data\n",
      "    dynamic_ema    : whether to include dynamic articulatory features\n",
      "                     (bool, default True)\n",
      "    sampling_rate  : sampling rate of the frames, in Hz (int, default 100)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use the <corpus>.abx.extract_h5_features function.\n",
    "\n",
    "help(mocha.abx.extract_h5_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we extract normalized articulatory features.\n",
    "\n",
    "mocha.abx.extract_h5_features(\n",
    "    ema_features='stds_byspeaker', output_name='mocha_ema'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we invert normalized acoustic features using the trained rnn.\n",
    "\n",
    "mocha.abx.extract_h5_features(\n",
    "    audio_features='mfcc_stds_byspeaker', inverter=rnn,\n",
    "    output_name='mocha_inv_rnn'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute ABX metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function abx_from_features in module ac2art.corpora.prototype.abx._abx:\n",
      "\n",
      "abx_from_features(features, fileset=None, byspeaker=True, limit_phones=False, n_jobs=1)\n",
      "    Run the ABXpy pipeline on a set of pre-extracted mocha features.\n",
      "    \n",
      "    features     : name of a h5 file of mocha features created with\n",
      "                   the `extract_h5_features` function (str)\n",
      "    fileset      : optional name of a fileset whose utterances'\n",
      "                   features to use (str)\n",
      "    byspeaker    : whether to discriminate pairs from the same\n",
      "                   speaker only (bool, default True)\n",
      "    limit_phones : whether to aggregate some phonemes, using\n",
      "                   the 'ipa_reduced' column of the mocha symbols\n",
      "                   file as mapping (bool, default False)\n",
      "    n_jobs       : number of CPU cores to use (positive int, default 1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use the <corpus>.abx.abx_from_features function.\n",
    "\n",
    "help(mocha.abx.abx_from_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using found /home/archeo/Bureau/stage_coml/datasets/processed_mocha_timit/abx/mocha_validation_byspk_task.abx file.\n",
      "ABXpy distance module was successfully run.\n",
      "ABXpy score module was successfully run.\n",
      "ABXpy analyze module was successfully run.\n",
      "Done running ABXpy. Results were written to '/home/archeo/Bureau/stage_coml/datasets/processed_mocha_timit/abx/mocha_ema_validation_byspk_abx.csv'.\n",
      "Replacing phoneme symbols with IPA ones...\n",
      "Done updating scores file.\n"
     ]
    }
   ],
   "source": [
    "# Here, we compute ABX metrics on the validation set\n",
    "# using the previously extracted true articulatory data.\n",
    "\n",
    "mocha.abx.abx_from_features(\n",
    "    features='mocha_ema', fileset='validation'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the computed ABX scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>n</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>phones</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>aɪ_iː</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aɪ_j</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aɪ_m</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aɪ_y</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aɪ_æ</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aɪ_ɑː</th>\n",
       "      <td>0.937500</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aɪ_ə</th>\n",
       "      <td>0.850000</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aɪ_ɛ</th>\n",
       "      <td>0.583333</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aɪ_ɛɪ</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aɪ_ɪ</th>\n",
       "      <td>0.833333</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aɪ_ɹ</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aʊ_n</th>\n",
       "      <td>0.888889</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aʊ_s</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aʊ_æ</th>\n",
       "      <td>0.250000</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aʊ_ɒ</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aʊ_ə</th>\n",
       "      <td>0.692308</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aʊ_ɪ</th>\n",
       "      <td>0.830000</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b_d</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b_dʒ</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b_f</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b_g</th>\n",
       "      <td>0.812500</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b_h</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b_j</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b_k</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b_l</th>\n",
       "      <td>0.985294</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b_m</th>\n",
       "      <td>0.625000</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b_n</th>\n",
       "      <td>0.916667</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b_p</th>\n",
       "      <td>0.684211</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b_s</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b_t</th>\n",
       "      <td>0.881579</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ɒ_ʌ</th>\n",
       "      <td>0.375000</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ɔɪ_ə</th>\n",
       "      <td>0.961538</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ɔː_ə</th>\n",
       "      <td>0.425000</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ɔː_ɛə</th>\n",
       "      <td>0.625000</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ɔː_ɪ</th>\n",
       "      <td>0.958333</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ɔː_ʌ</th>\n",
       "      <td>0.750000</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ə_əʊ</th>\n",
       "      <td>0.700000</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ə_ɛ</th>\n",
       "      <td>0.762821</td>\n",
       "      <td>156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ə_ɛə</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ə_ɛɪ</th>\n",
       "      <td>0.934211</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ə_ɜ:</th>\n",
       "      <td>0.781250</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ə_ɪ</th>\n",
       "      <td>0.693359</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ə_ɪə</th>\n",
       "      <td>0.750000</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ə_ʌ</th>\n",
       "      <td>0.594828</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>əʊ_ɪ</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>əʊ_ʃ</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ɛ_ɛɪ</th>\n",
       "      <td>0.583333</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ɛ_ɪ</th>\n",
       "      <td>0.790909</td>\n",
       "      <td>220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ɛ_ʃ</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ɛ_ʊ</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ɛ_ʌ</th>\n",
       "      <td>0.625000</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ɛə_ɪ</th>\n",
       "      <td>0.687500</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ɛɪ_ɜ:</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ɛɪ_ɪ</th>\n",
       "      <td>0.625000</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ɛɪ_ʊ</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ɜ:_ɪ</th>\n",
       "      <td>0.937500</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ɜ:_ʊ</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ɪ_ʊ</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ɪ_ʌ</th>\n",
       "      <td>0.683333</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ʃ_ʒ</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>302 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           score    n\n",
       "phones               \n",
       "aɪ_iː   1.000000    4\n",
       "aɪ_j    1.000000    4\n",
       "aɪ_m    1.000000    4\n",
       "aɪ_y    1.000000    4\n",
       "aɪ_æ    1.000000    4\n",
       "aɪ_ɑː   0.937500   16\n",
       "aɪ_ə    0.850000  100\n",
       "aɪ_ɛ    0.583333   12\n",
       "aɪ_ɛɪ   0.500000    8\n",
       "aɪ_ɪ    0.833333   24\n",
       "aɪ_ɹ    1.000000    4\n",
       "aʊ_n    0.888889  144\n",
       "aʊ_s    1.000000    4\n",
       "aʊ_æ    0.250000    4\n",
       "aʊ_ɒ    0.500000    4\n",
       "aʊ_ə    0.692308   52\n",
       "aʊ_ɪ    0.830000  100\n",
       "b_d     1.000000   24\n",
       "b_dʒ    1.000000   12\n",
       "b_f     1.000000   24\n",
       "b_g     0.812500   16\n",
       "b_h     1.000000   64\n",
       "b_j     1.000000    4\n",
       "b_k     1.000000    4\n",
       "b_l     0.985294   68\n",
       "b_m     0.625000   32\n",
       "b_n     0.916667   12\n",
       "b_p     0.684211   76\n",
       "b_s     1.000000   80\n",
       "b_t     0.881579   76\n",
       "...          ...  ...\n",
       "ɒ_ʌ     0.375000    8\n",
       "ɔɪ_ə    0.961538  104\n",
       "ɔː_ə    0.425000   40\n",
       "ɔː_ɛə   0.625000    8\n",
       "ɔː_ɪ    0.958333   72\n",
       "ɔː_ʌ    0.750000    4\n",
       "ə_əʊ    0.700000   20\n",
       "ə_ɛ     0.762821  156\n",
       "ə_ɛə    0.666667   12\n",
       "ə_ɛɪ    0.934211   76\n",
       "ə_ɜ:    0.781250   32\n",
       "ə_ɪ     0.693359  512\n",
       "ə_ɪə    0.750000    4\n",
       "ə_ʌ     0.594828  116\n",
       "əʊ_ɪ    0.500000   28\n",
       "əʊ_ʃ    1.000000    4\n",
       "ɛ_ɛɪ    0.583333   24\n",
       "ɛ_ɪ     0.790909  220\n",
       "ɛ_ʃ     1.000000    4\n",
       "ɛ_ʊ     0.500000    4\n",
       "ɛ_ʌ     0.625000    8\n",
       "ɛə_ɪ    0.687500   16\n",
       "ɛɪ_ɜ:   1.000000    8\n",
       "ɛɪ_ɪ    0.625000   48\n",
       "ɛɪ_ʊ    1.000000    4\n",
       "ɜ:_ɪ    0.937500   16\n",
       "ɜ:_ʊ    1.000000    4\n",
       "ɪ_ʊ     1.000000    4\n",
       "ɪ_ʌ     0.683333   60\n",
       "ʃ_ʒ     1.000000    4\n",
       "\n",
       "[302 rows x 2 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the <corpus>.abx.load_abx_scores function.\n",
    "# This function reads the produced csv file and aggregates scores on the fly.\n",
    "\n",
    "mocha.abx.load_abx_scores('mocha_ema_validation_byspk')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
